{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0af57e58",
   "metadata": {},
   "source": [
    "# Deterministic, Scalable Weighted Sampling with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d80e263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.testing import assertDataFrameEqual\n",
    "from qikits.sampling.weighted_sampling import weighted_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ac40e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/10 18:19:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"DeterministicWeightedSampling\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbfd193a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a sample DataFrame with 1,000,000 rows...\n",
      "Original DataFrame has 1000000 rows.\n",
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- weight: double (nullable = false)\n",
      " |-- data: string (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# This method is scalable to billions of rows. We'll create a sample DataFrame\n",
    "# with 1 million rows for this demonstration.\n",
    "# The real DataFrame should have:\n",
    "# - A column with a unique identifier (here, 'id').\n",
    "# - A column with the sampling weight (here, 'weight').\n",
    "print(\"Creating a sample DataFrame with 1,000,000 rows...\")\n",
    "df = (\n",
    "    spark.range(1_000_000)\n",
    "    .withColumn(\"id\", F.col(\"id\"))\n",
    "    .withColumn(\"weight\", F.rand(seed=42) * 100 + 1)\n",
    "    .withColumn(\"data\", F.concat(F.lit(\"record_\"), F.col(\"id\")))\n",
    ")\n",
    "\n",
    "print(f\"Original DataFrame has {df.count()} rows.\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c3f0b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully sampled 10 elements.\n",
      "Sampled DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+-------------+\n",
      "|id    |weight            |data         |\n",
      "+------+------------------+-------------+\n",
      "|861053|45.50996449144522 |record_861053|\n",
      "|377342|75.03059429328809 |record_377342|\n",
      "|664857|50.54429652966017 |record_664857|\n",
      "|337391|97.76573696149322 |record_337391|\n",
      "|507042|27.128717353831842|record_507042|\n",
      "|536173|78.52170144645547 |record_536173|\n",
      "|426275|93.9210825314597  |record_426275|\n",
      "|504778|49.232353180455526|record_504778|\n",
      "|687258|43.459986093404076|record_687258|\n",
      "|725749|95.8164756277254  |record_725749|\n",
      "+------+------------------+-------------+\n",
      "\n",
      "\n",
      "Running again with the same seed to prove determinism...\n",
      "Verifying that both sampled DataFrames are identical...\n",
      "✅ Verification successful: The samples are identical.\n"
     ]
    }
   ],
   "source": [
    "# --- Parameters ---\n",
    "# The number of elements to sample.\n",
    "k = 10\n",
    "# A string seed to ensure the sampling is deterministic.\n",
    "# Change this seed to get a different sample.\n",
    "sampling_seed = \"my-secret-seed-42\"\n",
    "\n",
    "sampled_df = weighted_sample(\n",
    "    df, k=k, weight_col=\"weight\", id_columns=\"id\", seed=sampling_seed\n",
    ")\n",
    "\n",
    "# Show the Results\n",
    "print(f\"\\nSuccessfully sampled {sampled_df.count()} elements.\")\n",
    "print(\"Sampled DataFrame:\")\n",
    "sampled_df.show(truncate=False)\n",
    "\n",
    "# To prove it's deterministic, let's run it again with the same seed.\n",
    "# The result will be identical.\n",
    "print(\"\\nRunning again with the same seed to prove determinism...\")\n",
    "rerun_sampled_df = weighted_sample(\n",
    "    df, k=k, weight_col=\"weight\", id_columns=\"id\", seed=sampling_seed\n",
    ")\n",
    "\n",
    "# Verify that both sampled DataFrames are identical.\n",
    "print(\"Verifying that both sampled DataFrames are identical...\")\n",
    "assertDataFrameEqual(sampled_df, rerun_sampled_df)\n",
    "print(\"✅ Verification successful: The samples are identical.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a62f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d137a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
